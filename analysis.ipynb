{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Modeling for static models\n",
    "**This notebook serves as the main analysis module in this project**\n",
    "\n",
    "Since the time series analysis is not the first priority, I will first establish static classification models such as tree models and non-time dependent survival models. I will look at the history of the patients before confirmation of prediabetes and assume that the state of the patients was determined at that moment.\n",
    "\n",
    "\n",
    "The notebook has the following sections:\n",
    "1. Data Processing and Tables Joining\n",
    "2. Data Overview\n",
    "3. Missing Data\n",
    "4. Preprocessing\n",
    "5. Feature Selection\n",
    "6. Resample Data\n",
    "7. Machine Learning Model\n",
    "8. Deep Learning Model\n",
    "\n",
    "For clarity: we name all the pre-diabetes patients with prefix pre,diabetes patients with prefix diab and the patients progressed from pre-diabetes to diabetes with prefix pre2Diab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import util.cleaning_tools as tools\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay,\\\n",
    "precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "from random import sample\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "import scipy.stats as stats\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyepr-parameter\n",
    "TIME_SPEC = 2\n",
    "CUT_OFF = '{year}-12-31'.format(year=2019-TIME_SPEC)\n",
    "log = {}\n",
    "OUT_PATH = f\"../output/george/spec-{TIME_SPEC}year/\"\n",
    "name_dict = None\n",
    "\n",
    "#define indicators id\n",
    "term_id = None\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data processing and tables joining\n",
    "This section includes steps:\n",
    "1. Read the patients and test data.\n",
    "2. Select the target tests(indicators).\n",
    "3. Join the tables.\n",
    "4. Only inlcude the tests within six month before diagnosis of pre-diabetes.\n",
    "5. Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Read the patients and test data\n",
    "Read the data from the output, here the patients tables contains the detailed information about the patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patients data\n",
    "patients = pd.read_csv(f'../tables/output/diab_patient-{TIME_SPEC}year.csv', index_col=0)\n",
    "\n",
    "# define the file path and tables path for file reading\n",
    "file_path = r'../DATAFILE'\n",
    "tid_to_eid_path = r'iams_entity_concept'\n",
    "labresult_cps_path = 'lis_cps_result_data'\n",
    "labresult_hms_path = 'lis_hms_result_data'\n",
    "\n",
    "# read the fragment files and concat them\n",
    "usecols = [\"pseudo_patient_key\", \"reference_dtm\", \"diff_in_hour_reference_dtm\", \"result_str\", \"entity_id\", \"si_unit\", \"si_numeric\"]\n",
    "labresult_cps = tools.fileReader(file_path, labresult_cps_path, usecols=usecols)\n",
    "labresult_hms = tools.fileReader(file_path, labresult_hms_path, usecols=usecols)\n",
    "tid_desc = tools.fileReader(r'../DATAFILE', \"iams_concept\") #term id description\n",
    "tid_to_eid = tools.fileReader(file_path, tid_to_eid_path)\n",
    "\n",
    "# the datafield of cps and hms are the same, so we can concate them.\n",
    "labresult = pd.concat([labresult_cps, labresult_hms])\n",
    "\n",
    "# # district information\n",
    "# cols = [\"pseudo_patient_key\", \"district_board\"]\n",
    "# district_mapp = tools.fileReader(file_path, \"map_pmi_district\")\n",
    "# emg = tools.fileReader(file_path, \"aeis_case_data\", usecols = cols)\n",
    "# oaa = tools.fileReader(file_path, \"opas_case_data\", usecols = cols)\n",
    "# ipt = tools.fileReader(file_path, \"ipas_case_data1\", usecols = cols)\n",
    "# district_record = pd.concat([emg, oaa, ipt])\n",
    "# district_record.replace(r'\"\"', np.nan, inplace=True)\n",
    "# district_record.dropna(how='any', inplace=True)\n",
    "# patient_district = district_record.groupby(\"pseudo_patient_key\").apply(lambda x : x.iloc[0])\n",
    "# patient_district.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Select the target tests(indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = pd.Series(term_id).rename('term_id')\n",
    "target_tid_mapping = pd.merge(target_id,tid_to_eid,how='left',on='term_id')\n",
    "labresult_filtered = pd.merge(labresult,target_tid_mapping,how='inner',on='entity_id')\n",
    "#replace the null value with np.nan\n",
    "labresult_filtered.replace(r'\"\"', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Join the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left join the patients information(id, pre hour and diab hour) with lab-result by the patient key to match all the tests for each patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#left join the patients tables and test tables \n",
    "patients_test = pd.merge(left=patients[[\"pseudo_patient_key\", \"pre_diff_hour\", \"diab_diff_hour\"]], \n",
    "                         right=labresult_filtered[[\"pseudo_patient_key\", \"term_id\", \"reference_dtm\", \"diff_in_hour_reference_dtm\", \"si_unit\",\"si_numeric\" ]], \n",
    "                         on=\"pseudo_patient_key\", \n",
    "                         how=\"left\")\n",
    "#rename\n",
    "patients_test.rename(columns = {\"diff_in_hour_reference_dtm\": \"test_diff_hour\", \"reference_dtm\": \"test_dtm\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate each tests with its mean value and pivot table so that each test stands for a single feaures.\n",
    "\n",
    "Set time limit for including features: the tests must lies within **6 months** before the prediabetes diagnosis and then use the *mean value* for each test in this period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# select the observations that the test is within six month before the \n",
    "# prediabetes diagnosis to three months after\n",
    "# patients_test_filtered = patients_test.query(\"test_diff_hour > (pre_diff_hour - 6 * 30 * 24) and test_diff_hour < (pre_diff_hour + 3 * 60 * 24)\")\n",
    "patients_test_filtered = patients_test.query(\"test_diff_hour <= pre_diff_hour and test_diff_hour > (pre_diff_hour - 6 * 30 * 24)\")\n",
    "# drop the observations that the test is later than diagnosis of diabetes\n",
    "# patients_test_filtered = patients_test_filtered.query(\"diab_diff_hour.isnull() or test_diff_hour < diab_diff_hour\", engine=\"python\")\n",
    "\n",
    "# only keep the patient id, term_id and test results\n",
    "patients_test_filtered = patients_test_filtered[[\"pseudo_patient_key\", \"term_id\", \"test_diff_hour\", \"si_numeric\"]]\n",
    "# casting type for si_numeric\n",
    "patients_test_filtered[\"si_numeric\"] = patients_test_filtered[\"si_numeric\"].astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by patients and termid and aggregate the test result with latest test value\n",
    "patients_test_group = patients_test_filtered\\\n",
    "    .sort_values(by=[\"pseudo_patient_key\", \"test_diff_hour\"])\\\n",
    "    .groupby([\"pseudo_patient_key\", \"term_id\"], as_index=False)\\\n",
    "    .agg({\"si_numeric\":\"mean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table pivot\n",
    "patients_features_pivoted = patients_test_group.pivot_table(index=\"pseudo_patient_key\", columns=\"term_id\", values=\"si_numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "patients_features_pivoted = patients_features_pivoted.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_features_pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all the tests out of interest\n",
    "patients_features_pivoted.rename(columns=name_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_features_pivoted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the dmcs variables with test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "dataset =  pd.merge(left=patients_features_pivoted, right=patients, how=\"inner\", on=\"pseudo_patient_key\") # join the with the patient information\n",
    "dataset = dataset.query(\"HBA1C < 6.4 or HBA1C.isnull()\", engine='python')\n",
    "dataset = dataset.query(\"cholesLDL_1 > 0 or cholesLDL_1.isnull()\", engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write to disk\n",
    "# dataset.to_csv(f\"../tables/output/dataset-{TIME_SPEC}year\")\n",
    "dataset = pd.read_csv(f\"../tables/output/dataset-{TIME_SPEC}year\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "\n",
    "This section report the overall statistic of the data which will inlcude following topics:\n",
    "1. Mean, standard deviance and portion of each class for each test features.\n",
    "2. F-test for each test features.\n",
    "3. Correlation among these significant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(f\"../tables/output/dataset-{TIME_SPEC}year\", index_col=0)\n",
    "# dataset.drop([\"Unnamed: 0.1\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test progression period time\n",
    "assert (dataset.diab_diff_hour - dataset.pre_diff_hour - dataset.prog_pd).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test progression class\n",
    "\n",
    "# null progression period should be class 0\n",
    "assert (dataset.query(\"prog_pd.isnull()\", engine='python').cls != 0).sum() == 0\n",
    "\n",
    "# progression period greater than time spectrum should be class 0\n",
    "h = TIME_SPEC * 365.25 * 24\n",
    "assert (dataset.query(\"prog_pd >= {hour}\".format(hour=h)).cls != 0).sum() == 0\n",
    "\n",
    "# progression period less than time sepctrum should be class 1\n",
    "assert (dataset.query(\"prog_pd < {hour}\".format(hour=h)).cls != 1).sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Chi Square test and T test for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = []\n",
    "std = []\n",
    "missing_rate = []\n",
    "pvalue = []\n",
    "indicators = [\"pre_age\", \"sex\"] + list(name_dict.values())\n",
    "pos_mean = []\n",
    "neg_mean = []\n",
    "pos_std = []\n",
    "neg_std = []\n",
    "\n",
    "for ind in indicators:\n",
    "    if ind == \"sex\":\n",
    "        temp = dataset[['sex', 'cls']]\n",
    "        male_0 = temp.query(\"sex == 'M' and cls == 0\").count()\n",
    "        male_1 = temp.query(\"sex == 'M' and cls == 1\").count()\n",
    "        female_0 = temp.query(\"sex == 'F' and cls == 0\").count()\n",
    "        female_1 = temp.query(\"sex == 'F' and cls == 1\").count()\n",
    "        result = stats.chi2_contingency([[male_0, female_0], [male_1, female_1]])\n",
    "        p_value = result[1]\n",
    "        pvalue.append(p_value)\n",
    "        mean.append(np.nan)\n",
    "        std.append(np.nan)\n",
    "        pos_mean.append(np.nan)\n",
    "        neg_mean.append(np.nan)\n",
    "        pos_std.append(np.nan)\n",
    "        neg_std.append(np.nan)\n",
    "    else:\n",
    "        temp = dataset[[ind, \"cls\"]]\n",
    "        temp = temp[temp[ind].notnull()]\n",
    "        t0 = temp.query(\"cls == 0\")[ind]\n",
    "        t1 = temp.query(\"cls == 1\")[ind]\n",
    "        result = stats.ttest_ind(t0, t1)\n",
    "        pvalue.append(result.pvalue)\n",
    "        mean.append(temp[ind].mean())\n",
    "        std.append(temp[ind].std())\n",
    "        pos_mean.append(t1.mean())\n",
    "        neg_mean.append(t0.mean())\n",
    "        pos_std.append(t1.std())\n",
    "        neg_std.append(t0.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stt = pd.DataFrame({\n",
    "    \"feauture\": indicators,\n",
    "    \"mean\": mean,\n",
    "    \"standard deviance\": std,\n",
    "    \"neg_mean\": neg_mean,\n",
    "    \"neg_std\": neg_std,\n",
    "    \"pos_mean\": pos_mean,\n",
    "    \"pos_std\": pos_std,\n",
    "    \"p-value\": pvalue,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Statistics of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the overall statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sex\n",
    "sex_stt = dataset.groupby([\"cls\", \"sex\"])[\"pseudo_patient_key\"].count().reset_index()\n",
    "sex_stt[\"portion\"] = sex_stt[\"pseudo_patient_key\"] / sex_stt[\"pseudo_patient_key\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi-square for gender\n",
    "\n",
    "print(\"p-value for sex is: \", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Missing value\n",
    "In this section, \n",
    "1. explore the missing data of each feature.\n",
    "2. exlcude invalid features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Explore the missing data of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_info =['pseudo_patient_key',\n",
    "            'pre_dtm', \n",
    "            'pre_diff_hour', \n",
    "            'sex',\n",
    "            'pre_age',\n",
    "#             'age_group',\n",
    "#             'district_board',\n",
    "           'cls']\n",
    "# select missingness less than 25% tets and HBA1C\n",
    "\n",
    "tests_name = list(name_dict.values())\n",
    "features = dataset.copy()[demo_info+tests_name]\n",
    "missing = features.isnull().sum()\n",
    "percent = features.isnull().sum() / features.isnull().count()\n",
    "valid = features.notnull().sum()\n",
    "missing_data = pd.concat([missing, valid,percent], axis=1, keys=[\"Missing\",\"Valid\", \"Missing_percent\"])\n",
    "missing_data.sort_values(\"Missing_percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to statistic result dataframe\n",
    "stt[\"Missing Rate\"] = missing_data.loc[indicators, \"Missing_percent\"].to_list()\n",
    "# write to disk\n",
    "stt.to_csv(os.path.join(OUT_PATH, \"tables\", \"overall_statistics.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Exclude invalid features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the features of missingness excessing 30% except for HBA1C\n",
    "valid_tests = missing_data.loc[tests_name].query(\"Missing_percent < 0.3\").index.to_list()\n",
    "if not 'HBA1C' in valid_tests:\n",
    "    valid_tests.append('HBA1C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imputation\n",
    "# df_train, df_test = train_test_split(ds, test_size=0.1, random_state=42)\n",
    "# lr = LogisticRegression()\n",
    "# numeric_col = tests_name + [\"pre_age\", \"sex\", \"pre_diff_hour\"]\n",
    "# # imp = IterativeImputer(estimator=lr, missing_values=np.nan,max_iter=10, verbose=2, imputation_order='roman', random_state=0)\n",
    "# imp = IterativeImputer(missing_values=np.nan, max_iter=10, verbose=2, imputation_order='roman', random_state=0)\n",
    "# df_train[numeric_col] = imp.fit_transform(df_train[numeric_col])\n",
    "# df_train[numeric_col] = df_train[numeric_col].clip(0)\n",
    "# df_test[numeric_col] = imp.transform(df_test[numeric_col])\n",
    "# df_test[numeric_col] = df_test[numeric_col].clip(0)\n",
    "# df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Preprocessing\n",
    "In this section, we will do:\n",
    "1. drop null value records.\n",
    "2. map sex to 0 and 1.\n",
    "3. normalize the data and write the scaler to disk.\n",
    "4. split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a shallow copy so that we won't mess up with the original dataset\n",
    "ds = dataset.copy()[demo_info + valid_tests]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Drop null value records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all the null value\n",
    "ds = ds.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Map sex to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sex_mapper = {'F':0, 'M':1}\n",
    "ds[\"sex\"] = ds[\"sex\"].apply(lambda x : sex_mapper[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Split the data\n",
    "We split the data stratified on the class so the test set and training set has the same positive rate, we don't want to touch test set in the middle of anything about training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(ds, test_size=0.1, random_state=42, stratify=ds[\"cls\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Normalize the data and permenant the scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data using RobustScaler()\n",
    "scaler = RobustScaler()\n",
    "df_train[valid_tests] = scaler.fit_transform(df_train[valid_tests])\n",
    "df_test[valid_tests] = scaler.transform(df_test[valid_tests])\n",
    "\n",
    "# save the scaler\n",
    "file = os.path.join(OUT_PATH, \"models\", \"scaler.pkl\")\n",
    "pickle.dump(scaler, open(file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[valid_tests + [\"pre_age\", \"sex\"]]\n",
    "y_train = df_train[\"cls\"]\n",
    "X_test = df_test[valid_tests + [\"pre_age\", \"sex\"]]\n",
    "y_test = df_test[\"cls\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "# write to result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Feature Selection\n",
    "In this section we implement feature selection by **Logistic regression with L1 penalty**, since L1 penalty will generate sparse coefficient matrix, it can be used for feature selection by excluding the features with 0 coefficient.\n",
    "\n",
    "we will do the following steps:\n",
    "1. grid search for optimal parameter.\n",
    "2. fit the model.\n",
    "3. select features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Grid search for optimal parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "grid = {\"C\": [0.001, 0.01, 0.1, 1, 10 ,100, 1000]}\n",
    "search = GridSearchCV(estimator=lr, param_grid=grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X_train, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(X_train.columns).reshape(1,-1)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_model = LogisticRegression(**search.best_params_, penalty='l1', solver='liblinear')\n",
    "gs_model.fit(X_train, y_train)\n",
    "coefficients = gs_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = np.abs(coefficients)\n",
    "valid_features = features[importance > 0]\n",
    "valid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_conf = {}\n",
    "fs_conf[\"C\"] = search.best_params_[\"C\"]\n",
    "fs_conf[\"valid_features\"] = valid_features\n",
    "log[\"feature_selection\"] = fs_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[valid_features]\n",
    "X_test = X_test[valid_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Resample the data\n",
    "I use random sampling method to balance the data. To elaborate the training, I decided to do both the oversampling and undersampling for the traning set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "X_train_os, y_train_os = ros.fit_resample(X_train, y_train)\n",
    "y_train_os.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uos = RandomUnderSampler(random_state=0)\n",
    "X_train_us, y_train_us = uos.fit_resample(X_train, y_train)\n",
    "y_train_us.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will establish evaluating metrics for  the model performance and by which we compare each model performance. Specifically, the section includes following steps:\n",
    "\n",
    "1. Set up metircs: precision, recall, f1-score, accuracy.\n",
    "2. Train the models and display each metrics.\n",
    "3. Discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Evaluating Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False negatives(fn), False positives(fn), True negatives(tn), True positives(tp)\n",
    "\n",
    "Accuracy: the percentage of predicted positives that were correctly classified > true_samples / total_samples \n",
    "\n",
    "Recall: the percentage of **actual** positives that were correctly classified > tp / (tp + fn)\n",
    "\n",
    "Precision: the percentage of **predicted** positives that were correctly classified > tp / (tp + fp) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model):\n",
    "    \n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    print(\"=========================================================\")\n",
    "    print(\"Metrics for model \" + model.__class__.__name__)\n",
    "    report = classification_report(y_test, y_pred_test, target_names=['no incidence', 'incidence ocurred'])\n",
    "    print(report)\n",
    "    with open('out.txt', 'a') as f:\n",
    "        print(\"=========================================================\", file=f)\n",
    "        print(\"Metrics for model \" + model.__class__.__name__, file=f)\n",
    "        print(report, file=f)\n",
    "        print(\"\\n\\n\", file=f)\n",
    "#     plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_test, labels=[0.0,1.0])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['no incidence', 'incidence ocurred'])\n",
    "    disp.plot(values_format='d')\n",
    "    fig = disp.figure_\n",
    "    fig.set_figwidth(8)\n",
    "    fig.set_figheight(8)\n",
    "    plt.grid(False)\n",
    "    plt.title(\"Confusion Matrix for \" + model.__class__.__name__, fontsize=12, fontweight='bold')\n",
    "    plt.savefig(\"Confusion Matrix for \" + model.__class__.__name__ + \".jpeg\", bbox_inches='tight', dpi=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc(model):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    y_test_score = model.decision_function(X_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.title(\"ROC\")\n",
    "    plt.plot(fpr, tpr, 'b', label='AUC = %0.4f' % roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1], 'r--')\n",
    "    plt.xlim([0, 1.0])\n",
    "    plt.ylim([0, 1.01])\n",
    "    plt.title(\"ROC for model \" + model.__class__.__name__)\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Training and evaluating\n",
    "\n",
    "1. Logistic Regression(baseline)\n",
    "2. Decision Tree\n",
    "3. RandomForestClassifier\n",
    "4. Ada Boost\n",
    "5. GradientBoostingClassifier\n",
    "6. VotingClassifier\n",
    "7. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1) Logistic Regression(oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "grid = {\"C\": [0.001, 0.01, 0.1, 1, 10 ,100, 1000], 'penalty':['l1', 'l2']}\n",
    "\n",
    "logreg_cv = GridSearchCV(logreg, grid, cv=5, scoring='balanced_accuracy')\n",
    "\n",
    "logreg_cv.fit(X_train_os, y_train_os)\n",
    "\n",
    "print(\"Best parameters \" , logreg_cv.best_params_)\n",
    "\n",
    "logreg2 = LogisticRegression(**logreg_cv.best_params_)\n",
    "logreg2.fit(X_train_os, y_train_os)\n",
    "get_scores(logreg2)\n",
    "plot_auc(logreg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2) Logistic Regression(Undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "grid = {\"C\": [0.001, 0.01, 0.1, 1, 10 ,100, 1000], 'penalty':['l1', 'l2']}\n",
    "\n",
    "logreg_cv = GridSearchCV(logreg, grid, cv=5, scoring='balanced_accuracy')\n",
    "\n",
    "logreg_cv.fit(X_train_us, y_train_us)\n",
    "\n",
    "print(\"Best parameters \" , logreg_cv.best_params_)\n",
    "\n",
    "logreg2 = LogisticRegression(**logreg_cv.best_params_)\n",
    "logreg2.fit(X_train_us, y_train_us)\n",
    "get_scores(logreg2)\n",
    "plot_auc(logreg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Decision Tree (oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train_os, y_train_os)\n",
    "get_scores(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1) Decision Tree (undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train_us, y_train_us)\n",
    "get_scores(dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) RandomForestClassifier(oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(class_weight={0.0:1,1.0:3}, random_state=42)\n",
    "rfc.fit(X_train_os, y_train_os)\n",
    "get_scores(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2) RandomForestClassifier(undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(class_weight={0.0:1,1.0:3}, random_state=42)\n",
    "rfc.fit(X_train_us, y_train_us)\n",
    "get_scores(rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1) AdaBoost(oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb = AdaBoostClassifier(n_estimators=100)\n",
    "adb.fit(X_train_os, y_train_os)\n",
    "get_scores(adb)\n",
    "plot_auc(adb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2) AdaBoost(undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adb = AdaBoostClassifier(n_estimators=100)\n",
    "adb.fit(X_train_us, y_train_us)\n",
    "get_scores(adb)\n",
    "plot_auc(adb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1) Gradient Tree Boosting(oversmapling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb= GradientBoostingClassifier(n_estimators=100, max_depth=1, random_state=0).fit(X_train_os, y_train_os)\n",
    "get_scores(xgb)\n",
    "plot_auc(xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2) Gradient Tree Boosting(undersmapling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb= GradientBoostingClassifier(n_estimators=100, max_depth=1, random_state=0).fit(X_train_us, y_train_us)\n",
    "get_scores(xgb)\n",
    "plot_auc(xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1) VotingClassifier(oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "VC = VotingClassifier(\n",
    "    estimators=[('xgb', xgb), ('adb', adb)],\n",
    "    voting=\"soft\",\n",
    "    weights=[2,1],\n",
    "    flatten_transform=True)\n",
    "VC = VC.fit(X_train_os, y_train_os)\n",
    "get_scores(VC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2) VotingClassifier(undersampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VC = VotingClassifier(\n",
    "    estimators=[('xgb', xgb), ('adb', adb)],\n",
    "    voting=\"soft\",\n",
    "    weights=[2,1],\n",
    "    flatten_transform=True)\n",
    "VC = VC.fit(X_train_us, y_train_us)\n",
    "get_scores(VC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "SVM = SVC(gamma=\"auto\").fit(X_train_us, y_train_us)\n",
    "get_scores(SVM)\n",
    "plot_auc(SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping callback on AUC\n",
    "class AUCStopping(keras.callbacks.Callback):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if(logs.get('val_auc') >= 0.81 and logs.get('val_recall') >= 0.9):\n",
    "            print(\"\\n Early stopping beacause validation auc excesses 80%\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callbacks: AUCStopping = AUCStopping()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the metrics and define the model creating method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'),\n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'), # we focus on recall metrics\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    keras.metrics.AUC(name='prc', curve='PR') # precision-recall curve\n",
    "]\n",
    "\n",
    "def make_model(metrics=METRICS, output_bias=None):\n",
    "    if output_bias is not None:\n",
    "        output_bias = tf.keras.initializers.Constant(output_bias)\n",
    "        \n",
    "    # build the model, this model can be seen as a logistic regression but with extra drop-out layers \n",
    "    # for avoiding overfitting     \n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(16, activation='relu', input_shape=(X_train.shape[-1],)),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dropout(0.5), # avoid overfitting\n",
    "        keras.layers.Dense(1, activation='sigmoid', bias_initializer=output_bias)\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=1e-3), #adam is not sensitive to different scale of loss\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=metrics\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 2000 # make sure each batch containes positive case\n",
    "total = y_test.count()\n",
    "pos = y_test.sum()\n",
    "neg = total - pos\n",
    "weight_0 = (1 / neg) * total / 2\n",
    "weight_1 = (1 / pos) * total / 2\n",
    "weight_0, weight_1\n",
    "CLASS_WEIGHT = {0:weight_0, 1:weight_1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model.evaluate(X_train, y_train, batch_size=2000, verbose=0)\n",
    "# print(\"loss: {:0.4f}\".format(results[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = make_model()\n",
    "baseline_history = model.fit(\n",
    "    X_train,\n",
    "    y_train.astype(np.int64),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks = [callbacks],\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign weight to different classes so that the model will focus on the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model = make_model()\n",
    "\n",
    "class_weight = {0: 1, 1: 2}\n",
    "weighted_history = weighted_model.fit(\n",
    "    X_train_us,\n",
    "    y_train_us.astype(np.int64),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks = [callbacks],\n",
    "    class_weight=class_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = (weighted_model.predict(X_test) > 0.5).astype(\"int\")\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = y_test.to_numpy().reshape(-1,1)\n",
    "(temp == y_test_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = (temp * (y_test_pred == 1)).sum()\n",
    "fn = (temp * (y_test_pred == 0)).sum()\n",
    "recall = tp / (tp+fn)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model.save(os.path.join(OUT_PATH,\"models\", \"weighted_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = {}\n",
    "model_conf[\"epochs\"] = EPOCHS\n",
    "model_conf[\"BATCH_SIZE\"] = BATCH_SIZE\n",
    "model_conf[\"weight\"] = class_weight\n",
    "dl_info = {}\n",
    "evaluation = weighted_model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=2)\n",
    "dl_info[\"model_conf\"] = model_conf\n",
    "dl_info[\"loss\"] = evaluation[0]\n",
    "dl_info[\"accuracy\"] = evaluation[4]\n",
    "dl_info[\"precision\"] = evaluation[5]\n",
    "dl_info[\"recall\"] = evaluation[6]\n",
    "dl_info[\"recall\"] = evaluation[7]\n",
    "log[\"dl_info\"] = dl_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = weighted_model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=2)\n",
    "df_info[\"model_conf\"] = model_conf\n",
    "dl_info[\"loss\"] = evaluation[0]\n",
    "dl_info[\"accuracy\"] = evaluation[4]\n",
    "dl_info[\"precision\"] = evaluation[5]\n",
    "dl_info[\"recall\"] = evaluation[6]\n",
    "dl_info[\"recall\"] = evaluation[7]\n",
    "log[\"dl_info\"] = dl_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUT_PATH, \"result.json\"), 'w', encoding='utf-8') as f:\n",
    "    json.dump(log, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Display Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sub section, I will display the metrics I set up against the training epochs for both baseline model and weighted model. This will include:\n",
    "1. Training Loss\n",
    "2. Recall\n",
    "3. Precision\n",
    "4. Accuracy\n",
    "\n",
    "And also I will investigate AUC for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model):\n",
    "    results = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "    for name, value in zip(model.metrics_names, results):\n",
    "        print(name, ': ', value)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(model)\n",
    "print_metrics(weighted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "y_pred = np.array([0.3 , 0.1, 0.1, 0.7])\n",
    "y_true = np.array([1,0,1,0])\n",
    "\n",
    "def recallLoss(y_true, y_pred):\n",
    "    threshold = 0.2\n",
    "    y_pred = (y_pred > threshold).astype(np.int32)\n",
    "    recall = y_pred[y_true == 1].mean()\n",
    "    return recall\n",
    "recallLoss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='fn'),\n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'), # we focus on recall metrics\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    keras.metrics.AUC(name='prc', curve='PR') # precision-recall curve\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([0.3 , 0.1])\n",
    "y_true = np.array([1,0])\n",
    "y_pred = (y_pred > 0.2).astype(np.int32)\n",
    "y_pred[y_true == 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "mpl.rcParams['figure.figsize'] = (12,10)\n",
    "def plot_metrics(history):\n",
    "    metrics = ['loss', 'accuracy', \"recall\", \"precision\"]\n",
    "    for n, metric in enumerate(metrics):\n",
    "        name = metric.replace(\"_\", \" \").capitalize()\n",
    "        plt.subplot(2, 2, n+1)\n",
    "        plt.plot(history.epoch, history.history[metric], color=colors[0], label=\"Train\")\n",
    "        plt.plot(history.epoch, history.history['val_'+metric], color=colors[0], linestyle=\"--\", label=\"Val\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(name)\n",
    "        if metric == 'loss':\n",
    "            plt.ylim([0, plt.ylim()[1]])\n",
    "        elif metric == 'auc':\n",
    "            plt.ylim([0.8,1])\n",
    "        else:\n",
    "            plt.ylim([0,1])\n",
    "        plt.legend()\n",
    "    plt.savefig(os.path.join(OUT_PATH, \"charts\", \"training_history.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(weighted_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_predictions_baseline = model.predict(X_train, batch_size=BATCH_SIZE)\n",
    "# test_predictions_baseline = model.predict(X_test, batch_size=BATCH_SIZE)\n",
    "train_predictions_weighted = weighted_model.predict(X_train, batch_size=BATCH_SIZE)\n",
    "test_predictions_weighted = weighted_model.predict(X_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model(os.path.join(OUT_PATH, \"models\", \"weighted_model\"))\n",
    "new_model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot ROC\n",
    "def plot_roc(name, labels, predictions, **kwargs):\n",
    "    fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
    "    plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
    "    plt.xlabel('False positives [%]')\n",
    "    plt.ylabel('True positives [%]')\n",
    "    plt.xlim([0,100])\n",
    "    plt.ylim([0,100])\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_roc(\"Train Baseline\", y_train, train_predictions_baseline, color=colors[0])\n",
    "# plot_roc(\"Test Baseline\", y_test, test_predictions_baseline, color=colors[0], linestyle='--')\n",
    "plot_roc(\"Train Weighted\", y_train, train_predictions_weighted, color=colors[0], linestyle='--')\n",
    "plot_roc(\"Test Weighted\", y_test, test_predictions_weighted, color=colors[1], linestyle='--')\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(os.path.join(OUT_PATH, \"charts\", \"Model ROC\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "y_pred = weighted_model.predict(X_test).ravel()\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "AUC = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "plt.plot([0,1],[0,1], 'k--')\n",
    "plt.plot(fpr, tpr, label=\"weighted model (area = {:.3f})\".format(AUC))\n",
    "plt.xlabel(\"True Positive Rate\")\n",
    "plt.ylabel(\"False Postive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,5))\n",
    "fig.axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix\n",
    "def plot_cm(y, predictions, threshold=0.5):\n",
    "    cm = confusion_matrix(y, predictions > threshold)\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['no incidence', 'incidence ocurred'])\n",
    "    disp.plot(ax=ax)\n",
    "    plt.title('Confusion matrix @{:.2f} threshold'.format(threshold))\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.savefig(\"./foo.png\")\n",
    "plot_cm(y_test, weighted_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.Feature permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from typing import Callable\n",
    "y_test_pred = weighted_model.predict(X_test)\n",
    "roc_auc_score(y_test, y_test_pred, average=\"weighted\")\n",
    "valid_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test[\"creatinine\"].sample(frac=1 ,replace=False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance(model: Callable, X: pd.DataFrame, y: pd.Series, n_repeats: int, metric_fn: Callable, **params) -> dict:\n",
    "    '''\n",
    "    the method return the mean score and std through out the n_repeat, the score is computed by the provided callable type\n",
    "    metric\n",
    "    \n",
    "    Assumption & Method\n",
    "    It assumes that the higher the score, the better the performance of the model is, and we use difference to measure the model\n",
    "    reliance of the feature.\n",
    "    \n",
    "    Arg:\n",
    "        model: model interface implements predicit method that return the decision score of the prediciton \n",
    "        X: input of validation set to be permutated\n",
    "        y: ground truth\n",
    "        n_repeats: the times of the iteration\n",
    "        metric_fn: the callable type to compute the score\n",
    "        params: addtional parameter to pass into the metric_funtion\n",
    "    \n",
    "    Return:\n",
    "        mean score over n repeats for each features.\n",
    "    '''\n",
    "    features = list(X.columns)\n",
    "    y_pred = model.predict(X)\n",
    "    original_score = metric_fn(y, y_pred, **params)\n",
    "    imp = {}\n",
    "    for _ in range(n_repeats):\n",
    "        for f in features:\n",
    "            X_new = X.copy() #copy the reference to the original dataframe\n",
    "            X_new[f] = X[f].sample(frac=1, replace=False).to_list()\n",
    "            y_pred = model.predict(X_new)\n",
    "            score = metric_fn(y, y_pred, **params)\n",
    "            diff = original_score - score\n",
    "            # assert the new score should not be greater than before\n",
    "            prev = imp.get(f, 0)\n",
    "            # add the new score difference\n",
    "            imp[f] = prev+ diff\n",
    "    for f in imp:\n",
    "        imp[f] = imp[f] / n_repeats\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_importance(weighted_model, X_test, y_test, 30, roc_auc_score, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-dependent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {\n",
    "    5200279: \"creatinineRenal\",\n",
    "    5200289: \"cholesHDL\",\n",
    "    5200290: \"choles\",\n",
    "    5200295: \"creatinine\",\n",
    "    5200305: \"glucose\",\n",
    "    5200306: \"fastingGlucose\",\n",
    "    5200325: \"triglyceride\",\n",
    "    5200406: \"cholesLDL_1\",\n",
    "    5201215: \"glucoseInBlood\",\n",
    "    5203289: \"proteinCreatinineRatio\" ,   \n",
    "    5200345: \"albumin\",\n",
    "    5200346: \"albumin24h\",\n",
    "    5200387: \"potassiumSerumOrPlasma\",\n",
    "    5200393: \"proteinUrine\",\n",
    "    5200394: \"proteinUrine24h\",\n",
    "    5200402: \"albuminCreatinineRatio\",\n",
    "    5200485: \"HBA1C\",\n",
    "    5200547: \"albuminUnspecifiedTime\",\n",
    "    5200679: \"cholesLDL_2\",\n",
    "    5200715: \"creatinineRenalClearance\",\n",
    "    5200935: \"microalbuminCreatinineRatio\",\n",
    "    5201051: \"proteinCreatinineMassRatio\",\n",
    "    5204348: \"glomerularFiltrationRate\"\n",
    "}\n",
    "test = list(name_dict.values())\n",
    "\n",
    "tests_dict = {5200289: \"cholesHDL\",\n",
    " 5200290: \"choles\",\n",
    " 5200295: \"creatinine\",\n",
    " 5200306: \"fastingGlucose\",\n",
    " 5200325: \"triglyceride\",\n",
    " 5200485: \"HBA1C\"\n",
    "}\n",
    "tests_id = list(tests_dict.keys())\n",
    "tests_name = list(tests_dict.values())\n",
    "demo_info =['pseudo_patient_key',\n",
    "            'pre_dtm', \n",
    "            'pre_diff_hour', \n",
    "            'sex',\n",
    "            'pre_age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patients data\n",
    "patients = pd.read_csv(r'../tables/output/group_patient_age.csv', index_col=0)\n",
    "\n",
    "# define the file path and tables path for file reading\n",
    "file_path = r'../DATAFILE'\n",
    "tid_to_eid_path = r'iams_entity_concept'\n",
    "labresult_cps_path = 'lis_cps_result_data'\n",
    "labresult_hms_path = 'lis_hms_result_data'\n",
    " \n",
    "# read the fragment files and concat them\n",
    "usecols = [\"pseudo_patient_key\", \"reference_dtm\", \"diff_in_hour_reference_dtm\", \"result_str\", \"entity_id\", \"si_unit\", \"si_numeric\"]\n",
    "labresult_cps = tools.fileReader(file_path, labresult_cps_path, usecols=usecols)\n",
    "labresult_hms = tools.fileReader(file_path, labresult_hms_path, usecols=usecols)\n",
    "tid_to_eid = tools.fileReader(file_path, tid_to_eid_path)\n",
    "\n",
    "# the datafield of cps and hms are the same, so we can concate them.\n",
    "labresult = pd.concat([labresult_cps, labresult_hms])\n",
    "\n",
    "# patients data\n",
    "patients = pd.read_csv(r'../tables/output/group_patient_age.csv', index_col=0)\n",
    "\n",
    "# define the file path and tables path for file reading\n",
    "file_path = r'../DATAFILE'\n",
    "tid_to_eid_path = r'iams_entity_concept'\n",
    "labresult_cps_path = 'lis_cps_result_data'\n",
    "labresult_hms_path = 'lis_hms_result_data'\n",
    " \n",
    "# read the fragment files and concat them\n",
    "usecols = [\"pseudo_patient_key\", \"reference_dtm\", \"diff_in_hour_reference_dtm\", \"result_str\", \"entity_id\", \"si_unit\", \"si_numeric\"]\n",
    "labresult_cps = tools.fileReader(file_path, labresult_cps_path, usecols=usecols)\n",
    "labresult_hms = tools.fileReader(file_path, labresult_hms_path, usecols=usecols)\n",
    "tid_to_eid = tools.fileReader(file_path, tid_to_eid_path)\n",
    "del labresult_cps\n",
    "del labresult_hms\n",
    "\n",
    "# the datafield of cps and hms are the same, so we can concate them.\n",
    "labresult = pd.concat([labresult_cps, labresult_hms])\n",
    "\n",
    "patients = patients.query(\"label != 2\")\n",
    "patients = patients.query(\"diab_age >= 18.0 or diab_age.isnull()\", engine=\"python\")\n",
    "\n",
    "eid = tid_to_eid[tid_to_eid.term_id.isin(tests_id)][\"entity_id\"]\n",
    "\n",
    "features=[\"pseudo_patient_key\", \"age\", 'test_name', 'si_numeric']\n",
    "\n",
    "# left join the table with the patients test\n",
    "dataset = pd.merge(left=patients, right=labresult[labresult.entity_id.isin(eid)], how='inner', on=\"pseudo_patient_key\")\n",
    "\n",
    "f = lambda x : int(x[:4])\n",
    "dataset = dataset.assign(age=dataset[\"reference_dtm\"].apply(f) - dataset[\"dob_Y\"].apply(f))\n",
    "# merge with tid\n",
    "dataset = pd.merge(left=dataset, right=tid_to_eid, how='inner', on=\"entity_id\")\n",
    "# map the name of the tests\n",
    "dataset[\"test_name\"] = dataset[\"term_id\"].apply(lambda x : tests_dict[x])\n",
    "# truncate the dataset at the moment of the prediabetes\n",
    "dataset = dataset.query(\"reference_dtm <= pre_dtm\")\n",
    "# exclude the patient later than 2016-12-31\n",
    "dataset = dataset.query(\"pre_dtm <= '2016-12-31'\")\n",
    "\n",
    "# feature columns\n",
    "f_col = dataset[features]\n",
    "f_col = f_col.replace(r'\"\"', np.nan) # mark \"\" as null value\n",
    "f_col[\"si_numeric\"] = f_col[\"si_numeric\"].astype('float')\n",
    "# group by patient and age\n",
    "test_mean = f_col.groupby([\"pseudo_patient_key\",\"age\", \"test_name\"], as_index=False).mean()\n",
    "\n",
    "test_mean.sort_values([\"pseudo_patient_key\", \"test_name\", \"age\"])\n",
    "\n",
    "# map the class for each patient\n",
    "right = dataset[[\"pseudo_patient_key\",\"label\"]].drop_duplicates()\n",
    "ds = pd.merge(left=test_mean, right=right, on=\"pseudo_patient_key\")\n",
    "# encode the class\n",
    "test_label_dict = {\"cholesHDL\":0,\n",
    "\"choles\":1,\n",
    "\"creatinine\":2,\n",
    "\"fastingGlucose\":3,\n",
    "\"triglyceride\":4,\n",
    " \"HBA1C\":5\n",
    "}\n",
    "ds[\"test_label\"] = ds[\"test_name\"].apply(lambda x : test_label_dict[x])\n",
    "# drop null value\n",
    "ds = ds.dropna(how=\"any\")\n",
    "\n",
    "# find the max length of test sequence\n",
    "max_len = ds.groupby(\"pseudo_patient_key\")[\"pseudo_patient_key\"].count().max()\n",
    "n_patient = ds.pseudo_patient_key.nunique()\n",
    "ds = ds.sort_values([\"pseudo_patient_key\",\"age\", \"test_name\"])\n",
    "\n",
    "\n",
    "patient_id = ds[\"pseudo_patient_key\"].unique()\n",
    "head = 0\n",
    "tail = 0\n",
    "n = ds.shape[0]\n",
    "def to_X(df):\n",
    "    n = df.shape[0]\n",
    "    max_len = df.groupby(\"pseudo_patient_key\")[\"pseudo_patient_key\"].count().max()\n",
    "    n_patient = df.pseudo_patient_key.nunique()\n",
    "    df = df.sort_values([\"pseudo_patient_key\",\"age\", \"test_name\"])\n",
    "    patient_id = df[\"pseudo_patient_key\"].unique()\n",
    "    X = np.zeros((n_patient, max_len, 3))\n",
    "    head = 0\n",
    "    tail = 0\n",
    "    y = np.zeros((n_patient,))\n",
    "    for i, id in enumerate(patient_id):\n",
    "        y[i] = df.iloc[tail][\"label\"]\n",
    "        while(df.iloc[tail][\"pseudo_patient_key\"] == id):\n",
    "            tail += 1\n",
    "            if tail == n:\n",
    "                break\n",
    "        diff = tail - head\n",
    "        X[i,:diff, :] = df.iloc[head:tail][[\"test_label\", \"age\", \"si_numeric\"]]\n",
    "        head = tail\n",
    "        if i % 100 == 0:\n",
    "            print(f\"finished {i}/{n_patient}\")\n",
    "    return X, y\n",
    "X , y = to_X(ds)   \n",
    "# split the data to test and train set\n",
    "# train_df, test_df = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='fn'),\n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'), # we focus on recall metrics\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    keras.metrics.AUC(name='prc', curve='PR') # precision-recall curve\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "EMB_DIM = 3 # the dimension of defining the state\n",
    "LSTM_UNITS = 128\n",
    "CLASS_NUM = 4\n",
    "MAX_LEN = 86\n",
    "model_lstm = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, strides=1, activation='relu', padding='causal', input_shape=[MAX_LEN,EMB_DIM]),\n",
    "    tf.keras.layers.LSTM(LSTM_UNITS),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5), # avoid overfitting\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dropout(0.5), # avoid overfitting\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dropout(0.5), # avoid overfitting\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    numerator = 2 * tf.reduce_sum(y_true * y_pred)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred)\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "model_lstm.compile(loss=keras.losses.BinaryCrossentropy(), optimizer='adam', metrics=METRICS)\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BTACH_SIZE = 10\n",
    "weight_minor = 1\n",
    "weight_major = 1000\n",
    "class_weight = {0: weight_major, 1: weight_minor}\n",
    "weighted_history = model_lstm.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data = (X_test, y_test),\n",
    "    class_weight=class_weight,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survival Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_info.remove(\"pre_age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_ds = dataset[valid_features.tolist() + demo_info + [\"diab_dtm\",\"label\", \"diab_diff_hour\"]]\n",
    "pre_dtm = pd.to_datetime(cox_ds[\"pre_dtm\"])\n",
    "cox_ds[\"prog_pd_hour\"] = (cox_ds[\"diab_diff_hour\"] - cox_ds[\"pre_diff_hour\"])\n",
    "#fill the na value with the hour difference between pre and 2019-12-16\n",
    "# we use 2019-12-16 as censored date to compromise the bias of date time set by HA\n",
    "cox_ds[\"prog_pd_hour\"] = cox_ds[\"prog_pd_hour\"].fillna((pd.to_datetime(\"2019-12-16\") - pre_dtm).dt.days * 24)\n",
    "# mapping sex {\"F\": 0, \"M\":1}\n",
    "sex_mapper = {\"F\": 0, \"M\":1}\n",
    "cox_ds[\"sex\"] = cox_ds[\"sex\"].apply(lambda x : sex_mapper[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_train = cox_ds[valid_features.tolist() + [\"sex\", \"prog_pd_hour\", \"label\"]]\n",
    "cox_train = cox_train.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_train.groupby(\"label\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "cox_train[valid_features.tolist()] = scaler.fit_transform(cox_train[tests_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cph = CoxPHFitter()\n",
    "cph.fit(cox_train, duration_col='prog_pd_hour', event_col='label')\n",
    "cph.print_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
